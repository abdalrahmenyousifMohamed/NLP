{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/Op+TOOv/Z4JOggyxZxeD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdalrahmenyousifMohamed/NLP/blob/main/attention_mechanism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "CyEvdovxdgOj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pSiLM_2dgL0",
        "outputId": "4422026a-288a-4ec6-b4c7-7e9133a58215"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('max seq : {}'.format(max(len(l) for l in X_train)))\n",
        "print('avg seq: {}'.format(sum(map(len, X_train))/len(X_train)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbfEGCjzdgJD",
        "outputId": "674e1d48-0477-4d28-cd2f-1fd636f6ce03"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max seq : 2494\n",
            "avg seq: 238.71364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 500\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)"
      ],
      "metadata": {
        "id": "gCgZkqIydgGj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bk2AYQYJUwM4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self,units):\n",
        "    super(BahdanauAttention,self).__init__()\n",
        "    self.W1 = Dense(units)\n",
        "    self.W2 = Dense(units)\n",
        "    self.V = Dense(1)\n",
        "\n",
        "  def call(self , values , query):\n",
        "      #However, key and value are the same\n",
        "\n",
        "      # query shape == (batch_size , hidden size)\n",
        "\n",
        "      # hidden_with_time_axis shape == (batch_size , 1 , hidden size)\n",
        "      # Change the dimension for the later addition to calculate the score\n",
        "\n",
        "      hidden_with_time_axis = tf.expand_dims(query , 1)\n",
        "\n",
        "      # score shape == (batch_size , max_length , 1)\n",
        "\n",
        "      # we get 1 at the last axis because we are applying score to self.V\n",
        "      # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "\n",
        "      score = self.V(tf.nn.tanh(\n",
        "          self.W1(values)  + self.W2(hidden_with_time_axis)\n",
        "      ))\n",
        "\n",
        "      # attention_weights shape == (batch_size , max_length , 1)\n",
        "\n",
        "      attention_weights = tf.nn.softmax(score , axis=1)\n",
        "\n",
        "      # context_vector shape after sum == (batch_size , hidden_size)\n",
        "\n",
        "      context_vector = attention_weights * values\n",
        "      context_vector = tf.reduce_sum(context_vector , axis=1)\n",
        "\n",
        "      return context_vector , attention_weights\n",
        "\n"
      ],
      "metadata": {
        "id": "0aIKVtZvVbRU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "4VLVVytsbQ_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BiLSTM with Attention Mechanism"
      ],
      "metadata": {
        "id": "g4IL9ZvYbTcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense , Embedding , Bidirectional , LSTM , Concatenate , Dropout\n",
        "\n",
        "from tensorflow.keras import Input , Model\n",
        "from tensorflow.keras import optimizers\n",
        "import os"
      ],
      "metadata": {
        "id": "AqgzMByDbZZQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.array_ops import sequence_mask\n",
        "\n",
        "sequence_input = Input(shape=(max_len,) , dtype='int32')\n",
        "\n",
        "embedded_sequences = Embedding(vocab_size , 128 ,\n",
        "input_length=max_len , mask_zero=True )(sequence_input)"
      ],
      "metadata": {
        "id": "x9bzETHnclu2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow._api.v2.nn import dropout\n",
        "lstm = Bidirectional(LSTM(64 , dropout=0.5 , return_sequences=True))(embedded_sequences)"
      ],
      "metadata": {
        "id": "DXFl6Xw1eaXd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ret state\n",
        "\n",
        "lstm , forward_h , forward_c , backward_h , backward_c = Bidirectional(\n",
        "    LSTM(64,dropout=0.5,return_sequences=True , return_state=True)\n",
        "\n",
        ")(lstm)"
      ],
      "metadata": {
        "id": "KGffjlxSe0bI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# size of each state\n",
        "lstm.shape , forward_h.shape , forward_c.shape , backward_h.shape , backward_c.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgfoGZWvfeGq",
        "outputId": "1d2edc12-ebca-44d6-a17b-bfcc7fd8050d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([None, 500, 128]),\n",
              " TensorShape([None, 64]),\n",
              " TensorShape([None, 64]),\n",
              " TensorShape([None, 64]),\n",
              " TensorShape([None, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case of each hidden state or cell state, it has 128 dimensions, and in the case of lstm, it has a size of (500 Ã— 128). This means that a hidden state vector with forward and backward directions connected exists for all views."
      ],
      "metadata": {
        "id": "XdAtzX49gums"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "when using a bidirectional LSTM , forward LSTM and backward LSTM each have a hidden state and cell state. To use the hidden state and cell state of a bidirectional LSTM , concatenate the states of the two LSTMs."
      ],
      "metadata": {
        "id": "-8A3hgEtg1u2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_h = Concatenate() ([forward_h , backward_h]) # hidden state\n",
        "state_c = Concatenate() ([forward_c , backward_c]) # cell state"
      ],
      "metadata": {
        "id": "XZhTZH-0hxp3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention = BahdanauAttention(64) # weight size definition\n",
        "context_vector  , attention_weights = attention(lstm , state_h)"
      ],
      "metadata": {
        "id": "uAegDGxQiV47"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense1 = Dense(20, activation=\"relu\")(context_vector)\n",
        "dropout = Dropout(0.5)(dense1)\n",
        "output = Dense(1, activation=\"sigmoid\")(dropout)\n",
        "model = Model(inputs=sequence_input, outputs=output)"
      ],
      "metadata": {
        "id": "e2fyolV3jFKl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "VMbU0TMUjZqk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs = 3, batch_size = 256, validation_data=(X_test, y_test), verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o543S_JDjeM-",
        "outputId": "5458b8af-2b72-447a-8bc0-d18e5f4f7a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "45/98 [============>.................] - ETA: 9:24 - loss: 0.6550 - accuracy: 0.6043"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sTFzsDiZjf2s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}